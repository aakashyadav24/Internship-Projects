{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxSUVAlNwm9fw+leEpBA1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakashyadav24/Internship-Projects/blob/main/3Skill_WineQualityPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cb282af"
      },
      "source": [
        "# Wine Quality Prediction – End-to-End Machine Learning Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9967fcfb"
      },
      "source": [
        "## TASK 1: Load and Understand the Dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53c7f27a"
      },
      "source": [
        "1. Import the required Python libraries:\n",
        "• pandas\n",
        "• numpy\n",
        "• seaborn\n",
        "• matplotlib\n",
        "• scikit-learn\n",
        "2. Load the dataset using pd.read_csv().\n",
        "3. Display the dataset to understand its structure:\n",
        "• Show the first 5 rows using head()\n",
        "• Show the last 5 rows using tail()\n",
        "• Show random rows using sample()\n",
        "4. Write a short explanation:\n",
        "• What type of data is present?\n",
        "• What does each row represent?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ddd8a56",
        "outputId": "dcac4e8a-db42-41bb-f277-34df4680cc71"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/winequality.csv', sep=',')\n",
        "\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nLast 5 rows of the dataset:\")\n",
        "print(df.tail())\n",
        "\n",
        "print(\"\\n5 random rows of the dataset:\")\n",
        "print(df.sample(5))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 rows of the dataset:\n",
            "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "0            7.4              0.70         0.00             1.9      0.076   \n",
            "1            7.8              0.88         0.00             2.6      0.098   \n",
            "2            7.8              0.76         0.04             2.3      0.092   \n",
            "3           11.2              0.28         0.56             1.9      0.075   \n",
            "4            7.4              0.70         0.00             1.9      0.076   \n",
            "\n",
            "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
            "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
            "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
            "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "\n",
            "   alcohol  quality  \n",
            "0      9.4        5  \n",
            "1      9.8        5  \n",
            "2      9.8        5  \n",
            "3      9.8        6  \n",
            "4      9.4        5  \n",
            "\n",
            "Last 5 rows of the dataset:\n",
            "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "1594            6.2             0.600         0.08             2.0      0.090   \n",
            "1595            5.9             0.550         0.10             2.2      0.062   \n",
            "1596            6.3             0.510         0.13             2.3      0.076   \n",
            "1597            5.9             0.645         0.12             2.0      0.075   \n",
            "1598            6.0             0.310         0.47             3.6      0.067   \n",
            "\n",
            "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
            "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
            "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
            "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
            "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
            "\n",
            "      alcohol  quality  \n",
            "1594     10.5        5  \n",
            "1595     11.2        6  \n",
            "1596     11.0        6  \n",
            "1597     10.2        5  \n",
            "1598     11.0        6  \n",
            "\n",
            "5 random rows of the dataset:\n",
            "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "1145            8.2              0.20         0.43             2.5      0.076   \n",
            "961             7.1              0.56         0.14             1.6      0.078   \n",
            "559            13.0              0.47         0.49             4.3      0.085   \n",
            "1463            6.9              0.63         0.01             2.4      0.076   \n",
            "839             6.0              0.50         0.04             2.2      0.092   \n",
            "\n",
            "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "1145                 31.0                  51.0  0.99672  3.53       0.81   \n",
            "961                   7.0                  18.0  0.99592  3.27       0.62   \n",
            "559                   6.0                  47.0  1.00210  3.30       0.68   \n",
            "1463                 14.0                  39.0  0.99522  3.34       0.53   \n",
            "839                  13.0                  26.0  0.99647  3.46       0.47   \n",
            "\n",
            "      alcohol  quality  \n",
            "1145     10.4        6  \n",
            "961       9.3        5  \n",
            "559      12.7        6  \n",
            "1463     10.8        6  \n",
            "839      10.0        5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb45bf4d"
      },
      "source": [
        "The dataset `winequality.csv` contains various chemical properties of red wines and their quality ratings.\n",
        "\n",
        "**Data Types Present:**\n",
        "All columns in the dataset are numerical (float or integer types). Specifically:\n",
        "*   **Fixed acidity, Volatile acidity, Citric acid, Residual sugar, Chlorides, Free sulfur dioxide, Total sulfur dioxide, Density, pH, Sulphates, Alcohol**: These columns represent various chemical properties of the wine and are continuous numerical features (float).\n",
        "*   **Quality**: This column represents the quality rating of the wine, which is an integer ranging from 3 to 8.\n",
        "\n",
        "**What each row represents:**\n",
        "Each row in the DataFrame `df` represents a unique sample of red wine. The values in each column for a given row describe the measured chemical attributes of that specific wine sample, along with its overall quality rating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ead6f2a"
      },
      "source": [
        "## TASK 2: Basic Data Inspection\n",
        "\n",
        "Using Python code, answer the following questions:\n",
        "• What are the column names in the dataset?\n",
        "• How many rows and columns are there?\n",
        "• What is the data type of each column?\n",
        "• Display summary statistics using describe()\n",
        "\n",
        "After running the code, write a short explanation:\n",
        "• Why is data inspection important before training any ML model?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9a0aea8",
        "outputId": "28375651-8a58-4545-e218-d432f88734c4"
      },
      "source": [
        "print(\"Column Names:\")\n",
        "print(df.columns)\n",
        "\n",
        "print(\"\\nShape of the DataFrame (Rows, Columns):\")\n",
        "print(df.shape)\n",
        "\n",
        "print(\"\\nData Types of Each Column:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Names:\n",
            "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
            "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
            "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
            "      dtype='object')\n",
            "\n",
            "Shape of the DataFrame (Rows, Columns):\n",
            "(1599, 12)\n",
            "\n",
            "Data Types of Each Column:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1599 entries, 0 to 1598\n",
            "Data columns (total 12 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   fixed acidity         1599 non-null   float64\n",
            " 1   volatile acidity      1599 non-null   float64\n",
            " 2   citric acid           1599 non-null   float64\n",
            " 3   residual sugar        1599 non-null   float64\n",
            " 4   chlorides             1599 non-null   float64\n",
            " 5   free sulfur dioxide   1599 non-null   float64\n",
            " 6   total sulfur dioxide  1599 non-null   float64\n",
            " 7   density               1599 non-null   float64\n",
            " 8   pH                    1599 non-null   float64\n",
            " 9   sulphates             1599 non-null   float64\n",
            " 10  alcohol               1599 non-null   float64\n",
            " 11  quality               1599 non-null   int64  \n",
            "dtypes: float64(11), int64(1)\n",
            "memory usage: 150.0 KB\n",
            "None\n",
            "\n",
            "Descriptive Statistics:\n",
            "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
            "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
            "mean        8.319637          0.527821     0.270976        2.538806   \n",
            "std         1.741096          0.179060     0.194801        1.409928   \n",
            "min         4.600000          0.120000     0.000000        0.900000   \n",
            "25%         7.100000          0.390000     0.090000        1.900000   \n",
            "50%         7.900000          0.520000     0.260000        2.200000   \n",
            "75%         9.200000          0.640000     0.420000        2.600000   \n",
            "max        15.900000          1.580000     1.000000       15.500000   \n",
            "\n",
            "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
            "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
            "mean      0.087467            15.874922             46.467792     0.996747   \n",
            "std       0.047065            10.460157             32.895324     0.001887   \n",
            "min       0.012000             1.000000              6.000000     0.990070   \n",
            "25%       0.070000             7.000000             22.000000     0.995600   \n",
            "50%       0.079000            14.000000             38.000000     0.996750   \n",
            "75%       0.090000            21.000000             62.000000     0.997835   \n",
            "max       0.611000            72.000000            289.000000     1.003690   \n",
            "\n",
            "                pH    sulphates      alcohol      quality  \n",
            "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
            "mean      3.311113     0.658149    10.422983     5.636023  \n",
            "std       0.154386     0.169507     1.065668     0.807569  \n",
            "min       2.740000     0.330000     8.400000     3.000000  \n",
            "25%       3.210000     0.550000     9.500000     5.000000  \n",
            "50%       3.310000     0.620000    10.200000     6.000000  \n",
            "75%       3.400000     0.730000    11.100000     6.000000  \n",
            "max       4.010000     2.000000    14.900000     8.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8bc8de"
      },
      "source": [
        "Data inspection is a critical first step before training any machine learning model for several reasons:\n",
        "\n",
        "1.  **Understanding Data Characteristics**: It provides an initial understanding of the dataset's structure, features, and target variable. This includes grasping the scale, distribution, and general patterns within the data.\n",
        "2.  **Identifying Potential Issues**: Through inspection, one can identify various problems that could negatively impact model performance:\n",
        "    *   **Incorrect Data Types**: Features might be loaded with inappropriate data types (e.g., numbers as strings), requiring conversion.\n",
        "    *   **Missing Values**: Missing data can cause errors or bias in models. Inspection helps locate these gaps and guide imputation or removal strategies.\n",
        "    *   **Outliers**: Extreme values can skew statistical analyses and model training. Identifying them is crucial for deciding whether to remove, transform, or cap them.\n",
        "    *   **Inconsistent Data**: Errors in data entry or collection can lead to inconsistencies (e.g., different spellings for the same category), which need to be standardized.\n",
        "    *   **Scale Differences**: Features often have different ranges of values. Inspection highlights these differences, indicating the need for scaling techniques (e.g., standardization or normalization) to ensure fair treatment by algorithms.\n",
        "3.  **Guiding Preprocessing Steps**: The insights gained from data inspection directly inform subsequent data preprocessing steps. For instance, knowing about outliers might lead to robust scaling methods, and understanding data distributions might guide transformation choices (e.g., logarithmic transformations for skewed data).\n",
        "4.  **Feature Engineering Opportunities**: By understanding the relationships and characteristics of features, one can identify opportunities for creating new, more informative features (feature engineering).\n",
        "5.  **Preventing Model Bias**: Unaddressed data issues can lead to biased models that perform poorly on new, unseen data. Thorough inspection helps mitigate these risks, leading to more robust and reliable models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fce780e"
      },
      "source": [
        "## TASK 3: Missing Values Analysis\n",
        "\n",
        "Steps to perform:\n",
        "1. Check missing values using:\n",
        "• isnull()\n",
        "• isnull().sum()\n",
        "2. Answer the following:\n",
        "• Are there any missing values in the dataset?\n",
        "• If missing values were present, how would you handle them in a real-world Machine Learning project?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5475e50",
        "outputId": "645f631e-d9d8-4bfe-ac3f-02ab56ee65c1"
      },
      "source": [
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            "fixed acidity           0\n",
            "volatile acidity        0\n",
            "citric acid             0\n",
            "residual sugar          0\n",
            "chlorides               0\n",
            "free sulfur dioxide     0\n",
            "total sulfur dioxide    0\n",
            "density                 0\n",
            "pH                      0\n",
            "sulphates               0\n",
            "alcohol                 0\n",
            "quality                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b6d1bf9"
      },
      "source": [
        "Based on the output from `df.isnull().sum()`, there are **no missing values** present in the `winequality.csv` dataset. Each column shows a count of `0` for missing values.\n",
        "\n",
        "### Handling Missing Values in a Real-World Machine Learning Project:\n",
        "\n",
        "Common strategies for handling missing values include:\n",
        "\n",
        "1.  **Removal:**\n",
        "    *   **Row-wise Deletion (`df.dropna()`):** If a small percentage of rows have missing values, deleting those rows might be acceptable. This is suitable when missingness is random and the amount of data lost is negligible. However, it can lead to significant data loss if many rows have missing values.\n",
        "    *   **Column-wise Deletion (`df.drop()`):** If a column has a very high percentage of missing values (e.g., >70-80%), it might be best to drop the entire column, as it provides little information. This is an extreme measure and should be used cautiously.\n",
        "\n",
        "2.  **Imputation:** Replacing missing values with estimated ones.\n",
        "    *   **Mean/Median Imputation:** Replace missing numerical values with the mean or median of the respective column. The mean is sensitive to outliers, while the median is more robust. This is a simple and common method but can reduce variance and distort relationships if not used carefully.\n",
        "    *   **Mode Imputation:** Replace missing categorical values with the mode (most frequent value) of the column. It can also be used for numerical data, especially if it's discrete.\n",
        "    *   **Forward Fill / Backward Fill (`df.fillna(method='ffill')`, `df.fillna(method='bfill')`):** Fill missing values with the previous or next valid observation. This is often used in time-series data.\n",
        "    *   **Regression Imputation:** Predict missing values using a regression model based on other features in the dataset. This is more sophisticated but can be computationally intensive and assumes relationships between variables.\n",
        "    *   **K-Nearest Neighbors (KNN) Imputation:** Impute missing values based on the values of the k-nearest neighbors in the dataset. This can capture complex relationships but is also more computationally expensive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "552089f7"
      },
      "source": [
        "## TASK 4: Exploratory Data Analysis (EDA)\n",
        "\n",
        "Perform EDA on the quality column.\n",
        "1. Print the value counts of different quality scores.\n",
        "2. Plot a count plot for the quality variable.\n",
        "3. Write 2–3 observations based on the plot.\n",
        "\n",
        "Explain:\n",
        "• What does this graph tell you about the dataset?\n",
        "• How does EDA help before model training?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "9b7e0096",
        "outputId": "32186151-b7c7-406e-81e6-efa271a790a3"
      },
      "source": [
        "print(\"Value counts for 'quality' column:\")\n",
        "print(df['quality'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='quality', data=df, palette='viridis', hue='quality', legend=False)\n",
        "plt.title('Distribution of Wine Quality')\n",
        "plt.xlabel('Quality Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value counts for 'quality' column:\n",
            "quality\n",
            "5    681\n",
            "6    638\n",
            "7    199\n",
            "4     53\n",
            "8     18\n",
            "3     10\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f327349"
      },
      "source": [
        "### Observations from the 'Quality' Distribution Plot:\n",
        "\n",
        "1.  **Imbalanced Distribution**: The plot clearly shows that the 'quality' column has a highly imbalanced distribution. The majority of wines are rated 5 or 6, which are considered average quality. There are significantly fewer wines rated with very low quality (3 and 4) or very high quality (7 and 8).\n",
        "2.  **Rarity of Extreme Quality Scores**: Wines with a quality score of 3, 4, or 8 are quite rare in this dataset. Specifically, quality scores of 3 and 8 have the lowest counts, making up a very small fraction of the total observations.\n",
        "3.  **Concentration around Average**: The highest concentration of wines falls within the average quality range, particularly scores 5 and 6, which together account for a substantial portion of the dataset.\n",
        "\n",
        "### What this graph tells about the dataset:\n",
        "\n",
        "This count plot provides critical insights into the target variable ('quality') of our dataset. It immediately highlights a class imbalance problem, which is very common in real-world datasets. For a classification task (especially if we convert 'quality' into binary classes like 'good' vs. 'bad'), this imbalance means that a model trained without addressing it might perform very well on the majority classes (quality 5 and 6) but poorly on the minority classes (quality 3, 4, 7, 8). This could lead to misleading accuracy metrics.\n",
        "\n",
        "### How EDA helps before model training:\n",
        "\n",
        "Exploratory Data Analysis (EDA) like this plot is crucial before model training for several reasons:\n",
        "\n",
        "*   **Identifies Data Imbalance**: As seen here, it quickly reveals if the target variable classes are imbalanced, which directly impacts model choice and evaluation metrics. For imbalanced datasets, techniques like oversampling, undersampling, or using specific algorithms (e.g., SMOTE, class weights) would be considered.\n",
        "*   **Guides Feature Engineering and Preprocessing**: Understanding the distribution helps in deciding if transformations are needed. For instance, if a feature is heavily skewed, a logarithmic transformation might be beneficial. For classification, identifying imbalance informs strategies to balance the dataset.\n",
        "*   **Helps in Model Selection**: Certain models perform better with balanced data or are more robust to imbalance. This insight guides the selection of appropriate algorithms.\n",
        "*   **Informs Evaluation Metrics**: For imbalanced data, simple accuracy is often insufficient. EDA helps to anticipate this and plan for more suitable metrics like precision, recall, F1-score, or AUC-ROC.\n",
        "*   **Reveals Data Quality Issues**: While not explicitly a quality issue here, extreme values or sparse categories might suggest data entry errors or require special handling.\n",
        "\n",
        "In summary, this plot tells us that our target variable is not uniformly distributed, and this understanding is vital for planning subsequent steps in the machine learning pipeline to build a robust and fair model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c3e2392"
      },
      "source": [
        "## TASK 5: Convert the Problem into a Classification Task\n",
        "\n",
        "To make the problem more practical, convert it into a binary classification problem:\n",
        "• Quality ≥ 7 → Good Wine (1)\n",
        "• Quality < 7 → Bad Wine (0)\n",
        "\n",
        "Steps:\n",
        "1. Create a new column called quality_label.\n",
        "2. Assign values based on the rules above.\n",
        "\n",
        "Write a short explanation:\n",
        "• Why is binary classification more useful than predicting exact quality scores in real-world systems?\n",
        "\n",
        "⸻\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff8e5791",
        "outputId": "b4661713-f1da-4a31-f93a-14953b73b1a7"
      },
      "source": [
        "df['quality_label'] = df['quality'].apply(lambda x: 1 if x >= 7 else 0)\n",
        "\n",
        "print(\"Value counts for 'quality_label' column:\")\n",
        "print(df['quality_label'].value_counts())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value counts for 'quality_label' column:\n",
            "quality_label\n",
            "0    1382\n",
            "1     217\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6c7a8b7"
      },
      "source": [
        "### Why Binary Classification is More Useful in Real-World Systems:\n",
        "\n",
        "Converting the multi-class 'quality' prediction into a binary classification problem (e.g., 'good' vs. 'bad' wine) is often more practical and useful in real-world applications for several reasons:\n",
        "\n",
        "1.  **Simplicity and Actionability**: In many business contexts, a simple \"good\" or \"bad\" (or \"buy\" vs. \"don't buy\", \"approve\" vs. \"reject\") decision is sufficient and more actionable than a nuanced score. For instance, a wine distributor might only care if a wine meets a certain quality threshold to be marketable, rather than its exact quality score.\n",
        "\n",
        "2.  **Robustness to Subjectivity**: Wine quality ratings, while given by experts, can still have some degree of subjectivity. Predicting an exact score from 3 to 8 is a fine-grained task that can be highly sensitive to slight variations in features or expert opinion. Binary classification aggregates these subtle differences into a more robust, broader category, reducing the impact of such subjectivity.\n",
        "\n",
        "3.  **Easier Interpretation**: Binary outcomes are inherently easier for non-technical stakeholders to understand and interpret. A clear \"good\" or \"bad\" label facilitates quicker decision-making and communication.\n",
        "\n",
        "4.  **Handling Imbalance**: As observed in the EDA, the original 'quality' scores are highly imbalanced, with very few wines at the extreme ends (scores 3, 4, 7, 8). While the binary classification still shows imbalance (many 'bad' wines, fewer 'good' wines), it consolidates the minority classes of \"very good\" wines (7 and 8) into a single, more substantial 'good' class. This can make the classification task slightly more manageable than trying to predict all individual scores.\n",
        "\n",
        "5.  **Focus on Key Decisions**: Often, the critical decision point revolves around a threshold. For example, a quality score of 7 might be considered the minimum for a premium wine. The binary classification directly targets this threshold, making the model more focused on the most important distinction for a given application.\n",
        "\n",
        "6.  **Performance Metrics**: For binary classification, well-established and intuitive metrics like precision, recall, F1-score, and AUC-ROC are readily available and provide clear insights into the model's performance on each class, especially when dealing with imbalance. Predicting exact scores (regression) would typically rely on metrics like RMSE or MAE, which can be harder to translate into business value for a subjective outcome like quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99da28a0"
      },
      "source": [
        "## TASK 6: Feature and Target Separation\n",
        "1. Separate the dataset into:\n",
        "• Features (X)\n",
        "• Target variable (y → quality_label)\n",
        "2. Explain clearly:\n",
        "• Why should the original quality column not be used as an input feature?\n",
        "\n",
        "⸻\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9d96e8d",
        "outputId": "adb5b2cb-c860-464c-bb16-96cf208c6858"
      },
      "source": [
        "X = df.drop(['quality', 'quality_label'], axis=1)\n",
        "y = df['quality_label']\n",
        "\n",
        "print(\"Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)\n",
        "print(\"\\nFirst 5 rows of X:\")\n",
        "print(X.head())\n",
        "print(\"\\nFirst 5 values of y:\")\n",
        "print(y.head())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of features (X): (1599, 11)\n",
            "Shape of target (y): (1599,)\n",
            "\n",
            "First 5 rows of X:\n",
            "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "0            7.4              0.70         0.00             1.9      0.076   \n",
            "1            7.8              0.88         0.00             2.6      0.098   \n",
            "2            7.8              0.76         0.04             2.3      0.092   \n",
            "3           11.2              0.28         0.56             1.9      0.075   \n",
            "4            7.4              0.70         0.00             1.9      0.076   \n",
            "\n",
            "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
            "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
            "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
            "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "\n",
            "   alcohol  \n",
            "0      9.4  \n",
            "1      9.8  \n",
            "2      9.8  \n",
            "3      9.8  \n",
            "4      9.4  \n",
            "\n",
            "First 5 values of y:\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: quality_label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796a534a"
      },
      "source": [
        "### Why the original 'quality' column should not be used as an input feature:\n",
        "\n",
        "Using the original 'quality' column as an input feature (predictor) to predict 'quality_label' would lead to a severe problem called **data leakage**.\n",
        "\n",
        "**Data Leakage** occurs when information from the target variable is inadvertently included in the features. In this case:\n",
        "\n",
        "1.  **Direct Relationship**: The `quality_label` column is directly derived from the `quality` column (quality >= 7 maps to 1, quality < 7 maps to 0). If `quality` were included in `X`, the model would have direct access to the answer before even trying to learn the underlying patterns from other features.\n",
        "2.  **Unrealistic Performance**: A model trained with `quality` as a feature would achieve unrealistically high accuracy, as it essentially already knows the target. This model would perform very poorly on new, unseen data where the `quality` score (which we are trying to predict the 'good'/'bad' label for) would not be available in advance.\n",
        "3.  **Loss of Generalizability**: The purpose of machine learning is to build a model that can generalize from observed data to unseen data. Including a feature that leaks information about the target prevents the model from learning genuine relationships between the other physicochemical properties and the wine quality. Instead, it would simply memorize the direct mapping from `quality` to `quality_label`.\n",
        "\n",
        "Therefore, to build a robust and generalizable model, the `quality` column must be excluded from the feature set `X`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1727cce0"
      },
      "source": [
        "## TASK 7: Train–Test Split\n",
        "\n",
        "Split the dataset into training and testing sets:\n",
        "• Training data: 80%\n",
        "• Testing data: 20%\n",
        "• Use random_state = 42\n",
        "\n",
        "Explain in your own words:\n",
        "• Why do we split data into training and testing sets?\n",
        "• What problem occurs if we train and test on the same data?\n",
        "\n",
        "⸻"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1411a52",
        "outputId": "4278398f-f1e4-4a4e-a49e-8d0a02f39229"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (1279, 11)\n",
            "Shape of X_test: (320, 11)\n",
            "Shape of y_train: (1279,)\n",
            "Shape of y_test: (320,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1fd883c"
      },
      "source": [
        "### Why Data Splitting is Important and Problems of Training and Testing on the Same Data:\n",
        "\n",
        "**Importance of Data Splitting:**\n",
        "\n",
        "Data splitting into training and testing sets is a fundamental practice in machine learning for several critical reasons:\n",
        "\n",
        "1.  **Evaluation of Generalization Ability**: The primary goal of a machine learning model is to generalize well to unseen data. By training the model on one subset (training set) and evaluating it on a completely separate, unseen subset (testing set), we can get an unbiased estimate of how the model will perform on new, real-world data.\n",
        "2.  **Detection of Overfitting**: Splitting the data helps in identifying overfitting. Overfitting occurs when a model learns the training data too well, including its noise and peculiarities, to the extent that it performs poorly on new data. If a model performs exceptionally well on the training data but poorly on the test data, it's a strong indicator of overfitting.\n",
        "3.  **Model Selection and Hyperparameter Tuning**: The test set provides a reliable benchmark for comparing different models or different hyperparameter configurations of the same model. It ensures that the chosen model and its parameters are not just good for the training data but are likely to perform well generally.\n",
        "4.  **Realistic Performance Assessment**: It provides a more realistic assessment of the model's performance in a real-world scenario, where the model will encounter data it has never seen before.\n",
        "\n",
        "**Problems Arising from Training and Testing on the Same Data:**\n",
        "\n",
        "If a model is trained and tested on the exact same dataset, several significant problems arise:\n",
        "\n",
        "1.  **Overoptimistic Performance Metrics**: The model will likely achieve very high accuracy or other performance metrics on the training data, as it has already 'seen' and learned the answers during training. This gives a highly misleading and overly optimistic impression of the model's true performance.\n",
        "2.  **Overfitting (Lack of Generalization)**: Training and testing on the same data almost guarantees that the model will overfit. It essentially memorizes the training examples rather than learning the underlying patterns. Such a model will fail dramatically when presented with new data.\n",
        "3.  **Invalid Model Selection**: If models are compared based on their performance on the training data, an overfit model might be mistakenly chosen as the best performer, leading to poor real-world deployment.\n",
        "4.  **No Insight into Real-World Efficacy**: Without a separate test set, there is no way to know how well the model will perform in a practical application with unseen data. The reported performance will not be representative of its true capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b58d7a7b"
      },
      "source": [
        "## TASK 8: Feature Scaling\n",
        "1. Apply StandardScaler to scale the numerical features.\n",
        "2. Ensure scaling is done correctly:\n",
        "• Fit on training data\n",
        "• Transform both training and test data\n",
        "3. Write an explanation:\n",
        "• Why is feature scaling important?\n",
        "• Which ML models need scaling and why?\n",
        "\n",
        "Relate your answer to distance-based and gradient-based algorithms.\n",
        "\n",
        "⸻"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa00d537",
        "outputId": "1b927f8a-fd4c-4046-e7d8-78d60610706c"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Shape of scaled X_train:\", X_train_scaled.shape)\n",
        "print(\"Shape of scaled X_test:\", X_test_scaled.shape)\n",
        "print(\"\\nFirst 5 rows of scaled X_train:\")\n",
        "print(X_train_scaled[:5])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of scaled X_train: (1279, 11)\n",
            "Shape of scaled X_test: (320, 11)\n",
            "\n",
            "First 5 rows of scaled X_train:\n",
            "[[ 0.21833164  0.88971201  0.19209222  0.30972563 -0.04964208  0.69100692\n",
            "   1.04293362  1.84669643  1.09349989  0.45822284  1.12317723]\n",
            " [-1.29016623 -1.78878251  0.65275338 -0.80507963 -0.45521361  2.38847304\n",
            "   3.59387025 -3.00449133 -0.40043872 -0.40119696  1.40827174]\n",
            " [ 1.49475291 -0.78434707  1.01104539 -0.52637831  0.59927236 -0.95796016\n",
            "  -0.99174203  0.76865471 -0.07566946  0.51551749 -0.58738978]\n",
            " [ 0.27635078  0.86181102 -0.06383064 -0.66572897 -0.00908493  0.01202048\n",
            "  -0.71842739  0.08948842  0.05423824 -1.08873281 -0.96751578]\n",
            " [ 0.04427419  2.81487994 -0.62686095  2.39998549 -0.31326357 -0.47296984\n",
            "   0.2229897   1.1998714   0.37900751 -0.9741435  -0.49235828]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7e9c614"
      },
      "source": [
        "### Importance of Feature Scaling:\n",
        "\n",
        "Feature scaling is a crucial preprocessing step in machine learning, especially for certain types of algorithms, for the following reasons:\n",
        "\n",
        "1.  **Equal Contribution of Features**: Features often have different ranges and units. Without scaling, features with larger numerical ranges might dominate the learning process, leading the model to give disproportionately more weight to those features, regardless of their actual predictive power. Scaling ensures that all features contribute equally to the distance calculations or gradient updates.\n",
        "2.  **Faster Convergence for Gradient-Based Models**: Algorithms that use gradient descent (e.g., Logistic Regression, Support Vector Machines, Neural Networks) converge much faster when features are scaled. If features have very different scales, the cost function will have an elongated, narrow shape, making it difficult for the optimizer to find the minimum efficiently. Scaling transforms the cost function into a more spherical shape, allowing gradient descent to take more direct steps towards the minimum.\n",
        "3.  **Improved Performance for Distance-Based Models**: Algorithms that rely on distance metrics (e.g., K-Nearest Neighbors (KNN), K-Means clustering, Support Vector Machines with RBF kernel) are heavily affected by the scale of features. If one feature has a much larger range than others, it will disproportionately influence the distance calculation. For instance, in KNN, a feature with values from 1 to 1000 will have a much greater impact on the Euclidean distance than a feature with values from 0 to 1, effectively overshadowing other potentially important features. Scaling ensures that all features contribute proportionally to the distance.\n",
        "4.  **Avoiding Numerical Instability**: Large differences in feature scales can sometimes lead to numerical instability issues during computation, especially with floating-point arithmetic. Scaling helps to keep values within a more manageable numerical range.\n",
        "\n",
        "###  Which ML models need scaling and why?\n",
        "KNN, SVM, Logistic Regression, Neural Networks → because they rely on distances or gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2166ed67"
      },
      "source": [
        "## TASK 9: Model Training\n",
        "\n",
        "Train and test the following Machine Learning models:\n",
        "1. Logistic Regression\n",
        "2. K-Nearest Neighbors (KNN)\n",
        "3. Decision Tree Classifier\n",
        "4. Random Forest Classifier\n",
        "5. Support Vector Machine (SVM)\n",
        "\n",
        "Use scaled or unscaled data correctly based on the model.\n",
        "\n",
        "Write 1–2 lines for each model explaining:\n",
        "• How the model works at a basic level.\n",
        "\n",
        "⸻"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09e2117a",
        "outputId": "5141546d-52b9-459c-abc4-fd76f1e98044"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "print(\"Classifier models imported successfully.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier models imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3c693c4",
        "outputId": "e647a2c8-9843-4d3e-99b2-09d1ad28e1f1"
      },
      "source": [
        "log_reg_model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "log_reg_model.fit(X_train_scaled, y_train)\n",
        "print(\"Logistic Regression model trained successfully.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb9fdc04",
        "outputId": "6e2672f8-e08e-44e4-8db6-1ab7de6f61b7"
      },
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "print(\"K-Nearest Neighbors model trained successfully.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Nearest Neighbors model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de9b1c85",
        "outputId": "481fbb79-7220-4b79-d6ed-7292c143c17e"
      },
      "source": [
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train_scaled, y_train)\n",
        "print(\"Decision Tree Classifier model trained successfully.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b2f9864",
        "outputId": "2482a8e6-1f85-477b-ebcc-ed5056514049"
      },
      "source": [
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "print(\"Random Forest Classifier model trained successfully.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "510def2a",
        "outputId": "9c8f9e02-4859-4937-920e-c1653b920dd8"
      },
      "source": [
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "print(\"Support Vector Machine model trained successfully.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Machine model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bd80dbe"
      },
      "source": [
        "### Basic Explanation of Each Model:\n",
        "\n",
        "1.  **Logistic Regression**:\n",
        "    *   **How it works**: Despite its name, Logistic Regression is a linear *classification* algorithm used to estimate the probability of a binary outcome. It models the probability of a default class (e.g., 'good wine') as a function of the input features. It uses a sigmoid (logistic) function to squash the output of a linear equation into a probability score between 0 and 1. If this probability crosses a certain threshold (typically 0.5), it classifies the instance into the positive class (1); otherwise, it classifies it into the negative class (0).\n",
        "\n",
        "2.  **K-Nearest Neighbors (KNN)**:\n",
        "    *   **How it works**: KNN is a non-parametric, lazy learning algorithm used for both classification and regression. For classification, it works by finding the 'k' closest data points (neighbors) in the training data to a new, unseen data point. The new data point is then assigned the class label that is most common among its 'k' nearest neighbors. The 'distance' between points is usually calculated using metrics like Euclidean distance.\n",
        "\n",
        "3.  **Decision Tree Classifier**:\n",
        "    *   **How it works**: A Decision Tree is a flowchart-like structure where each internal node represents a test on an attribute (feature), each branch represents the outcome of the test, and each leaf node represents a class label (decision). It works by recursively splitting the dataset into subsets based on the most significant attribute at each node, aiming to create increasingly pure subsets (subsets where most instances belong to the same class). Prediction for a new instance involves traversing the tree from the root to a leaf node by answering the attribute tests along the way.\n",
        "\n",
        "4.  **Random Forest Classifier**:\n",
        "    *   **How it works**: Random Forest is an ensemble learning method that builds multiple Decision Trees and merges their predictions to get a more accurate and stable prediction. It works by constructing a 'forest' of many Decision Trees during training. For classification, each individual tree in the forest outputs a class prediction, and the class that gets the most votes from all trees becomes the model's final prediction. This ensemble approach helps to reduce overfitting and improve robustness compared to a single Decision Tree.\n",
        "\n",
        "5.  **Support Vector Machine (SVM)**:\n",
        "    *   **How it works**: SVM is a powerful and versatile algorithm that can be used for both classification and regression. For classification, SVM works by finding an optimal hyperplane that best separates the data points of different classes in a high-dimensional space. The goal is to maximize the margin (the distance between the hyperplane and the nearest data points from each class, called support vectors). For non-linearly separable data, SVM uses kernel tricks to map the data into a higher-dimensional space where a linear separation might be possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dae4001e"
      },
      "source": [
        "# TASK 10: Model Evaluation and Comparison\n",
        "1. Calculate and print the accuracy of each model.\n",
        "2. Create a comparison table with:\n",
        "• Model Name\n",
        "• Accuracy\n",
        "3. Answer the following:\n",
        "• Which model performed the best?\n",
        "• Why do you think this model performed better on this dataset?\n",
        "\n",
        "⸻"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5775a083",
        "outputId": "65154bec-76b5-40d5-a4ba-5bffb85c5c7a"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the models dictionary\n",
        "models = {\n",
        "    \"Logistic Regression\": log_reg_model,\n",
        "    \"K-Nearest Neighbors\": knn_model,\n",
        "    \"Decision Tree\": dt_model,\n",
        "    \"Random Forest\": rf_model,\n",
        "    \"Support Vector Machine\": svm_model\n",
        "}\n",
        "\n",
        "model_accuracies = {\n",
        "    \"Model\": [],\n",
        "    \"Accuracy\": []\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    model_accuracies[\"Model\"].append(name)\n",
        "    model_accuracies[\"Accuracy\"].append(accuracy)\n",
        "\n",
        "accuracy_df = pd.DataFrame(model_accuracies)\n",
        "accuracy_df = accuracy_df.sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"Model Comparison Table:\")\n",
        "print(accuracy_df)\n",
        "\n",
        "best_model_name = accuracy_df.loc[0, 'Model']\n",
        "best_model_accuracy = accuracy_df.loc[0, 'Accuracy']\n",
        "\n",
        "print(f\"\\nBest performing model: {best_model_name} with an accuracy of {best_model_accuracy:.4f}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Comparison Table:\n",
            "                    Model  Accuracy\n",
            "0           Random Forest  0.900000\n",
            "1     K-Nearest Neighbors  0.881250\n",
            "2           Decision Tree  0.871875\n",
            "3     Logistic Regression  0.865625\n",
            "4  Support Vector Machine  0.853125\n",
            "\n",
            "Best performing model: Random Forest with an accuracy of 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b5ce45a"
      },
      "source": [
        "### Explanation for Random Forest's Superior Performance:\n",
        "\n",
        "The **Random Forest Classifier** emerged as the best-performing model with an accuracy of 0.9000. Its superior performance can be attributed to several key characteristics of ensemble learning, particularly when dealing with datasets like `winequality.csv`:\n",
        "\n",
        "1.  **Ensemble Learning (Bias-Variance Trade-off)**: Random Forest is an ensemble method that builds multiple decision trees during training. Each tree is trained on a random subset of the training data (bagging) and considers only a random subset of features for splitting at each node. This strategy significantly reduces overfitting, which is a common problem with individual decision trees. By averaging or majority-voting the predictions of many decorrelated trees, Random Forest achieves a lower variance than a single decision tree without a significant increase in bias.\n",
        "\n",
        "2.  **Robustness to Noise and Outliers**: The dataset, while clean in terms of missing values, might contain some noise or outliers in the feature values. Random Forest is inherently more robust to these imperfections because the impact of noise or outliers on any single tree's prediction is mitigated by the collective decision of the forest.\n",
        "\n",
        "3.  **Handling Non-linear Relationships**: Wine quality is likely influenced by complex, non-linear interactions between various physicochemical properties. Decision trees, the base learners in Random Forest, are capable of capturing these non-linear relationships. By combining many such trees, Random Forest can model highly intricate decision boundaries effectively.\n",
        "\n",
        "4.  **Feature Importance**: Random Forest implicitly performs feature selection and can handle a large number of features without explicit dimensionality reduction. It naturally ranks feature importance, which can be beneficial in understanding which chemical properties are most critical for determining wine quality.\n",
        "\n",
        "5.  **Reduced Overfitting**: While individual decision trees are prone to overfitting, the ensemble nature of Random Forest, particularly the random sampling of data and features, ensures that the overall model generalizes well to unseen data. This is crucial for a dataset where the boundary between 'good' and 'bad' wine might not be perfectly linear or simple.\n",
        "\n",
        "In summary, the Random Forest's ability to combine the strengths of multiple decision trees, reduce variance through bagging and feature randomness, and effectively capture complex relationships likely contributed to its higher accuracy compared to simpler models like Logistic Regression or less robust models like a single Decision Tree on this specific wine quality dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17103d48"
      },
      "source": [
        "## TASK 11: Pipeline and Hyperparameter Tuning\n",
        "1. Create a Machine Learning pipeline using:\n",
        "• StandardScaler\n",
        "• Logistic Regression or Support Vector Machine\n",
        "2. Apply GridSearchCV to tune hyperparameters such as:\n",
        "• C\n",
        "• kernel (for SVM)\n",
        "3. Print:\n",
        "• Best parameters\n",
        "• Best cross-validation score\n",
        "\n",
        "Explain:\n",
        "• Why pipelines are used in real-world ML systems.\n",
        "• Why hyperparameter tuning improves model performance.\n",
        "\n",
        "⸻"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0771e90",
        "outputId": "2ff4f500-d664-4719-affd-aca7503d0060"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"Pipeline and GridSearchCV imported successfully.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline and GridSearchCV imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc50d32c",
        "outputId": "18d43cc8-f2df-43fd-ebd7-3be1869a636d"
      },
      "source": [
        "log_reg_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logisticregression', LogisticRegression(random_state=42))\n",
        "])\n",
        "\n",
        "print(\"Logistic Regression Pipeline created successfully.\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Pipeline created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "334ec4c5",
        "outputId": "549f1df3-7d31-4c33-bf84-b4b27ac7151d"
      },
      "source": [
        "param_grid = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'logisticregression__solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "print(\"Hyperparameter grid for Logistic Regression pipeline defined successfully.\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter grid for Logistic Regression pipeline defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aa9a9e9",
        "outputId": "1217dd31-ad4a-4eea-a469-dcd3504a2393"
      },
      "source": [
        "grid_search = GridSearchCV(log_reg_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(\"GridSearchCV initialized successfully.\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GridSearchCV initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efcd9079",
        "outputId": "315242c7-5916-444a-8ce3-b547ce34c67b"
      },
      "source": [
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"GridSearchCV fitting completed.\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GridSearchCV fitting completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9c70144",
        "outputId": "d3a1926e-7639-496e-8e94-dd40383d12e6"
      },
      "source": [
        "print(\"Best Parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "print(\"\\nBest Cross-validation Accuracy:\")\n",
        "print(f\"{grid_search.best_score_:.4f}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters found by GridSearchCV:\n",
            "{'logisticregression__C': 0.1, 'logisticregression__solver': 'liblinear'}\n",
            "\n",
            "Best Cross-validation Accuracy:\n",
            "0.8858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "133d52ef"
      },
      "source": [
        "### Utility of Pipelines in Real-World ML Systems:\n",
        "\n",
        "Machine Learning Pipelines are incredibly useful in real-world systems for several compelling reasons:\n",
        "\n",
        "1.  **Streamlined Workflow**: Pipelines allow for the sequential application of multiple data transformations and a final estimator. This encapsulates the entire preprocessing and modeling workflow into a single object, making the code cleaner, more organized, and easier to manage.\n",
        "2.  **Prevents Data Leakage**: One of the most critical benefits is preventing data leakage. Preprocessing steps (like scaling or imputation) often fit parameters based on the training data. If these steps are applied to the entire dataset *before* splitting, information from the test set can 'leak' into the training process, leading to overly optimistic performance estimates. Pipelines ensure that transformations like `StandardScaler`'s `fit()` method are only called on the training data, and then `transform()` is applied consistently to both training and test data.\n",
        "3.  **Reproducibility**: By defining a clear, ordered sequence of operations, pipelines enhance the reproducibility of machine learning experiments. Anyone can easily replicate the exact steps taken to transform data and train a model.\n",
        "4.  **Easier Hyperparameter Tuning**: Pipelines make hyperparameter tuning across multiple steps much simpler. Tools like `GridSearchCV` or `RandomizedSearchCV` can optimize hyperparameters not only for the final estimator but also for the preprocessing steps (e.g., choice of imputer, scaling method). This is achieved by using parameter names prefixed with the step name (e.g., `'logisticregression__C'` as seen in our example).\n",
        "5.  **Deployment**: For deploying models, a pipeline allows the entire preprocessing and prediction logic to be packaged and deployed as a single, consistent unit. This reduces the risk of discrepancies between training and deployment environments.\n",
        "\n",
        "### How Hyperparameter Tuning Enhances Model Performance:\n",
        "\n",
        "Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model, which significantly enhances its performance:\n",
        "\n",
        "1.  **Optimizing Model Fit**: Models have parameters that are learned from data (e.g., coefficients in Logistic Regression) and hyperparameters that are set *before* training (e.g., `C` in Logistic Regression/SVM, `n_neighbors` in KNN, `max_depth` in Decision Trees). Hyperparameter tuning aims to find the values for these pre-set parameters that allow the model to best capture the underlying patterns in the data.\n",
        "2.  **Avoiding Underfitting and Overfitting**: Poorly chosen hyperparameters can lead to underfitting (model is too simple to capture the data's complexity) or overfitting (model learns the training data too well, including noise, and performs poorly on unseen data). Tuning helps strike the right balance, allowing the model to generalize well to new data.\n",
        "3.  **Improved Generalization**: By systematically searching for the best combination of hyperparameters, we can configure a model that achieves the highest possible performance on unseen data, leading to better generalization and more reliable predictions in real-world scenarios.\n",
        "4.  **Unlocking Model Potential**: Every model has a theoretical maximum performance given a specific dataset. Hyperparameter tuning helps unlock this potential by configuring the model in its most effective state. For instance, `GridSearchCV` exhaustively searches through a defined parameter space to find the combination that yields the best cross-validation score, ensuring we are using a well-configured version of our chosen algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5de41ae2"
      },
      "source": [
        "## TASK 12: Final Conclusion\n",
        "\n",
        "Write a short conclusion covering:\n",
        "• Understanding of the dataset\n",
        "• Important observations from EDA\n",
        "• Best performing model\n",
        "• What you learned from this project\n",
        "• How this project is similar to real-world Machine Learning applications\n",
        "\n",
        "⸻"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fa38c5c"
      },
      "source": [
        "## Project Conclusion\n",
        "\n",
        "This project involved building and evaluating several machine learning models to predict wine quality based on its physicochemical properties, transitioning from a regression-like problem to a binary classification task.\n",
        "\n",
        "### 1. Understanding of the Dataset\n",
        "The `winequality.csv` dataset comprised various continuous numerical features representing the chemical composition of red wines (e.g., fixed acidity, alcohol content) and an integer target variable, `quality`, ranging from 3 to 8. Each row represented a unique red wine sample. The dataset was clean, with no missing values, simplifying the initial data preparation.\n",
        "\n",
        "### 2. Important Observations from EDA\n",
        "Exploratory Data Analysis (EDA) revealed a significant imbalance in the original `quality` ratings. The majority of wines fell into the average quality scores of 5 and 6, while extreme quality scores (3, 4, 7, 8) were considerably less frequent. This observation was crucial, prompting the decision to convert the problem into a binary classification task (`Good Wine` if quality ≥ 7, `Bad Wine` if quality < 7) to make the problem more practical and address the class imbalance for model training.\n",
        "\n",
        "### 3. Best Performing Model\n",
        "Among the initial models tested (Logistic Regression, K-Nearest Neighbors, Decision Tree, Random Forest, and Support Vector Machine), the **Random Forest Classifier** demonstrated the best performance with an accuracy of **0.9000** on the scaled test data. This superior performance is likely due to its ensemble nature, which reduces overfitting and captures complex, non-linear relationships more effectively than individual models.\n",
        "\n",
        "Hyperparameter tuning of the Logistic Regression model within a pipeline (using `StandardScaler` and `GridSearchCV`) further improved its cross-validation accuracy to 0.8858, showing the value of optimization even for simpler models.\n",
        "\n",
        "### 4. What I learned from this project\n",
        "*   **Importance of EDA**: Early data inspection and EDA are vital for understanding data characteristics, identifying issues like class imbalance, and guiding subsequent preprocessing steps.\n",
        "*   **Data Transformation**: Converting a multi-class problem into a binary one can simplify the task and make it more actionable in real-world scenarios, especially when class distributions are highly skewed.\n",
        "*   **Feature Scaling**: For distance-based and gradient-based algorithms (like KNN, SVM, Logistic Regression), feature scaling is crucial for preventing features with larger ranges from dominating and for accelerating convergence.\n",
        "*   **Train-Test Split**: Strictly separating training and testing data is fundamental to accurately assess a model's generalization ability and prevent overfitting.\n",
        "*   **Pipelines**: Machine Learning pipelines streamline workflows, prevent data leakage, enhance reproducibility, and simplify hyperparameter tuning across multiple preprocessing and modeling steps.\n",
        "*   **Hyperparameter Tuning**: Optimizing hyperparameters is essential to unlock a model's full potential and improve its performance on unseen data.\n",
        "\n",
        "### 5. Similarity to Real-World Machine Learning Applications\n",
        "This project mirrors many aspects of real-world ML applications:\n",
        "*   **Business Problem Formulation**: Transforming a nuanced quality rating into a binary 'good'/'bad' decision is common in industries where clear, actionable insights are needed.\n",
        "*   **Data Preprocessing**: Handling data inspection, missing values (even if none were found here, the discussion is relevant), data type conversion, and feature scaling are standard initial steps.\n",
        "*   **Model Selection and Evaluation**: Trying multiple models and comparing their performance is a typical approach to find the most suitable algorithm for a given task.\n",
        "*   **Addressing Data Challenges**: Recognizing and addressing class imbalance, as done by converting to binary classification, is a frequent real-world challenge.\n",
        "*   **Robust Workflow**: Using pipelines and hyperparameter tuning are best practices for building robust, deployable, and high-performing ML systems, ensuring consistent data handling and optimal model configuration from development to deployment."
      ]
    }
  ]
}